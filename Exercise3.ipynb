{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikoczernin/neural_network_implementation/blob/main/Exercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wdumcrxk30S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import hashlib\n",
        "import requests\n",
        "import tarfile\n",
        "from datetime import datetime\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
        "from keras.applications import MobileNetV2\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "from skimage import feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5P4CXsmkTq8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download_dataset(url, download_dir, md5_checksum=None):\n",
        "    # Create download directory if it doesn't exist\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "\n",
        "    # Extract file name from URL\n",
        "    filename = os.path.basename(url)\n",
        "\n",
        "    # Download the file\n",
        "    filepath = os.path.join(download_dir, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        print(\"Downloading\", filename)\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    else:\n",
        "        print(\"File\", filename, \"already exists.\")\n",
        "\n",
        "    if md5_checksum is not None:\n",
        "      # Verify MD5 checksum\n",
        "      with open(filepath, 'rb') as f:\n",
        "          md5 = hashlib.md5()\n",
        "          while chunk := f.read(4096):\n",
        "              md5.update(chunk)\n",
        "      if md5.hexdigest() != md5_checksum:\n",
        "          print(\"MD5 checksum verification failed!\")\n",
        "          return None\n",
        "\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def extract_tar_gz(tar_gz_file, extract_dir):\n",
        "    # Extract the tar.gz file\n",
        "    print(\"Extracting\", tar_gz_file)\n",
        "    with tarfile.open(tar_gz_file, 'r:gz') as tar:\n",
        "        tar.extractall(path=extract_dir)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "injjpmttpLRf"
      },
      "outputs": [],
      "source": [
        "# function for displaying an image\n",
        "def show(img):\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "o0YKnuub56v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvjKO0i-mz62"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4xSvN6Om0Fh"
      },
      "source": [
        "## Labeled Faces in the Wild\n",
        "https://vis-www.cs.umass.edu/lfw/  \n",
        "Download: http://vis-www.cs.umass.edu/lfw/lfw.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwrKwQu-kczE",
        "outputId": "638f6738-1bf7-4749-e874-9b7d71fc79e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading lfw.tgz\n",
            "Extracting ./data/lfw.tgz\n",
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download the labeled faces in the wild\n",
        "url = \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\"\n",
        "md5_checksum = \"a17d05bd522c52d84eca14327a23d494\"\n",
        "download_dir = \"./data\"\n",
        "extract_dir = \"./data\"\n",
        "\n",
        "# Download the dataset\n",
        "dataset_filepath = download_dataset(url, download_dir, md5_checksum)\n",
        "\n",
        "# Extract the dataset\n",
        "if dataset_filepath:\n",
        "    extract_tar_gz(dataset_filepath, extract_dir)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsK5XjSCnHFF",
        "outputId": "914f18fb-0849-4e23-95b2-004c230a8ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1680 people with at least\n",
            "      2 in the dataset, giving us a total of\n",
            "      9164 images.\n"
          ]
        }
      ],
      "source": [
        "# now, in the folder \"./data/lfw/\" are dozens of directories\n",
        "# the dirs are labelled the name of the people who's photos they contain\n",
        "faces_location = \"data/lfw\"\n",
        "\n",
        "# out of the 5749 people, only 1680 have two or more images\n",
        "# we will only use those 1680 people's images\n",
        "min_number_of_images = 2\n",
        "\n",
        "faces_x_paths = [] # list/array of faces' image paths\n",
        "faces_y = [] # list/array of faces' labels/names\n",
        "\n",
        "\n",
        "for person_name in os.listdir(faces_location):\n",
        "  # get a list of all images in this name's directory\n",
        "  person_images = os.listdir(f\"{faces_location}/{person_name}\")\n",
        "  # if it exceeds the minimum count of images per face\n",
        "  if len(person_images) >= min_number_of_images:\n",
        "    for filename_image in person_images:\n",
        "      # save the path to the image in faces_x\n",
        "      faces_x_paths.append(f\"{faces_location}/{person_name}/{filename_image}\")\n",
        "      # save the label/name of the person in faces_y\n",
        "      faces_y.append(person_name)\n",
        "\n",
        "# turn the datalists into np.arrays\n",
        "faces_x_paths = np.array(faces_x_paths)\n",
        "faces_y = np.array(faces_y)\n",
        "\n",
        "print(f\"\"\"There are {np.unique(faces_y).shape[0]} people with at least\n",
        "      {min_number_of_images} in the dataset, giving us a total of\n",
        "      {faces_y.shape[0]} images.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc1ZRTORrK2N",
        "outputId": "e0c17fea-d9e7-4787-d81e-691bbfdfa15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coloured faces images: (9164, 250, 250, 3)\n",
            "Grayscale faces images: (9164, 250, 250)\n"
          ]
        }
      ],
      "source": [
        "# load the faces' image data into memory\n",
        "faces_x = []\n",
        "\n",
        "for image_path in faces_x_paths:\n",
        "  img = cv2.imread(image_path)\n",
        "  faces_x.append(img)\n",
        "\n",
        "# turn the datalist into a np.array\n",
        "faces_x = np.array(faces_x)\n",
        "\n",
        "print(\"Coloured faces images:\", faces_x.shape)\n",
        "\n",
        "# some algorithms will require B&W images\n",
        "faces_x_bw = []\n",
        "for image in faces_x:\n",
        "  image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces_x_bw.append(image_bw)\n",
        "\n",
        "# turn the datalist into a np.array\n",
        "faces_x_bw = np.array(faces_x_bw)\n",
        "\n",
        "print(\"Grayscale faces images:\", faces_x_bw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UftubGr4lKp5"
      },
      "source": [
        "## Zalando's Fashion-MNIST dataset\n",
        "@online{xiao2017/online,\n",
        "  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},\n",
        "  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},\n",
        "  date         = {2017-08-28},\n",
        "  year         = {2017},\n",
        "  eprintclass  = {cs.LG},\n",
        "  eprinttype   = {arXiv},\n",
        "  eprint       = {cs.LG/1708.07747},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boi7SjDylAcf",
        "outputId": "175f53f5-2ed2-42c1-91d3-38f339cb147e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training Images Shape: (60000, 28, 28)\n",
            "Training Labels Shape: (60000,)\n",
            "Test Images Shape: (10000, 28, 28)\n",
            "Test Labels Shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(fashion_x_train, fashion_y_train), (fashion_x_test, fashion_y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training Images Shape:\", fashion_x_train.shape)\n",
        "print(\"Training Labels Shape:\", fashion_y_train.shape)\n",
        "print(\"Test Images Shape:\", fashion_x_test.shape)\n",
        "print(\"Test Labels Shape:\", fashion_y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1-klAhpiL8"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Binary Patterns\n",
        "https://pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-opencv/"
      ],
      "metadata": {
        "id": "esi6bbTc9Xme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# images should be a list/array of cv2.images, they should be 1 channel (black and white)\n",
        "def lbp_feature_extraction(images, num_points=24, radius=8, time_it=False):\n",
        "  start_time = datetime.now()\n",
        "\n",
        "  print(\"Local Binary Patterns feature extraction starting\")\n",
        "\n",
        "  # list of image_features\n",
        "  # this will be the output x_train, i.e. the data\n",
        "  features = []\n",
        "\n",
        "  # iterate through the images\n",
        "  for img in images:\n",
        "    # transform the image to the lbp form\n",
        "    # this returns a matrix the same shape as the image\n",
        "    # with new pixel values in lbp representation\n",
        "    img_lbp = feature.local_binary_pattern(img, num_points, radius, method=\"uniform\")\n",
        "    # get a histogram representation of these values\n",
        "    (img_lbp_histogram, _) = np.histogram(img_lbp.ravel(), bins=np.arange(0, num_points + 3), range=(0, num_points + 2))\n",
        "    # normalize the histogram to fit between 0 and 1\n",
        "    img_lbp_histogram = img_lbp_histogram.astype(\"float\")\n",
        "    img_lbp_histogram /= (img_lbp_histogram.sum() + 1e-7)\n",
        "    # save the img_histogram as a feature\n",
        "    features.append(img_lbp_histogram)\n",
        "\n",
        "  # turn the list into a numpy matrix\n",
        "  features = np.vstack(features)\n",
        "\n",
        "  time_elapsed = datetime.now() - start_time\n",
        "  print('LBP-extraction done — time taken (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
        "\n",
        "  # return the transformed data\n",
        "  # if the parameter time_it is true, return the elapsed time as well\n",
        "  if time_it:\n",
        "    return features, time_elapsed\n",
        "  else:\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "mQehX8r39cp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "show()\n",
        "lbp_feature_extraction(faces_x_bw[0:1])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "9KjX8fE44Y1B",
        "outputId": "22cf0bc1-2b41-49ac-b790-68fb4f5229d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Binary Patterns feature extraction starting\n",
            "LBP-extraction done — time taken (hh:mm:ss.ms) 0:00:00.106921\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Invalid shape (26,) for image data",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b2c6a9b5561c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbp_feature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_x_bw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-7d7ebc9becb9>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# function for displaying an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0minterpolation_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         resample=None, url=None, data=None, **kwargs):\n\u001b[0;32m-> 2695\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    709\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 710\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    711\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (26,) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjBUqLvXpkU4"
      },
      "source": [
        "## SIFT (Scale-Invariant Feature Transform)\n",
        "\n",
        "http://weitz.de/sift/\n",
        "\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/#:~:text=The%20SIFT%20technique%20involves%20generating,feature%20matching%20and%20object%20recognition.\n",
        "\n",
        "Inspiration for the implementation:  \n",
        "https://github.com/Akhilesh64/Image-Classification-using-SIFT/blob/main/main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqIaaZ_wlEdy"
      },
      "outputs": [],
      "source": [
        "# function for SIFT-feature extraction\n",
        "# this returns the 1x128 array descriptors of all found features in the image\n",
        "def CalcFeatures(img):\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "  kp, des = sift.detectAndCompute(img, None)\n",
        "  return des\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_U8rJRRFVh4"
      },
      "outputs": [],
      "source": [
        "# Bag-Of-Features\n",
        "# now we assign the features of the training data to the clusters they are most\n",
        "# similar to. In other words: We put each feature into a bag of features.\n",
        "# We do so using a function\n",
        "# the function returns a k-long array, giving the number of features that\n",
        "# each cluster represents in the given features.\n",
        "# When this function is executed on many features, you get an overview on\n",
        "# how the clusters are represented.\n",
        "# When you pass only a single feature, you just get\n",
        "# its assigned cluster number as an index.\n",
        "def bag_of_features(features_array, clusters):\n",
        "  # number of clusters\n",
        "  k = clusters.shape[0]\n",
        "  # create a zeros vector, as long as the number of clusters k\n",
        "  # this will be\n",
        "  cluster_counter = np.zeros((1, k))\n",
        "  # iterate through all image features\n",
        "  for i, feature in enumerate(features_array):\n",
        "    # get the differences from the current feature to all cluster centres\n",
        "    differences = np.tile(feature, (k, 1)) - clusters\n",
        "    # get the euclidian distances -> sqrt(sum(differences squared))\n",
        "    distances = pow(((pow(differences, 2)).sum(axis=1)), 0.5)\n",
        "    # get the indices of the sorted distances to get the minimal distance\n",
        "    index_distances = distances.argsort()\n",
        "    # get the index of the minimum distance\n",
        "    index_min_distance = index_distances[0]\n",
        "    # for the closest cluster, increase the counter by 1\n",
        "    cluster_counter[0][index_min_distance] += 1\n",
        "  # return the numbers of assignments to the clusters\n",
        "  return cluster_counter\n",
        "\n",
        "# this executes the bag_of_features function, but only for 1 feature\n",
        "# it returns the index of the assigned cluster\n",
        "def get_bag_of_feature_assignment(feature, clusters):\n",
        "  cluster_counter = bag_of_features(np.array([feature]), clusters)\n",
        "  return np.where(cluster_counter == 1)[1][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVlDK4vkjb1j"
      },
      "outputs": [],
      "source": [
        "# this function uses SIFT-feature extraction to transform 1D-image data\n",
        "\n",
        "def SIFT_feature_extraction(x_train, y_train, x_test=None, y_test=None, k=200, time_it=True):\n",
        "  start_time = datetime.now()\n",
        "\n",
        "  print(\"SIFT feature extraction starting\")\n",
        "  print(\"Analysing features of training data\")\n",
        "\n",
        "  # if no separate test has given, we have to make a split later\n",
        "\n",
        "  # put all feature descriptors of all images in a matrix\n",
        "  # the matrix has 128 columns and 1 row for each feature of each image\n",
        "  # e.g. if each of 100 images has 3 features, the matrix will be 300 x 128\n",
        "  features = []\n",
        "\n",
        "  # do that only with the training data\n",
        "  for img in x_train:\n",
        "    img_des = CalcFeatures(img)\n",
        "    if img_des is not None:\n",
        "      features.append(img_des)\n",
        "\n",
        "  # 1 row per feature, 1 col per feature dimension\n",
        "  features = np.vstack(features)\n",
        "\n",
        "  print(f\"{features.shape[0]} features keypoints have been found within the {x_train.shape[0]} images.\")\n",
        "\n",
        "  # many the thousands of features in 128 dimensions are very similar\n",
        "  # also, two nearly identical objects will have some features that are\n",
        "  # very similar, but not the same.\n",
        "  # therefore we will create clusters for the features and use them as labels\n",
        "  # instead of the features themselves\n",
        "  # the number of clusters is k\n",
        "  # we use kmeans clustering by cv2\n",
        "\n",
        "  # stopping criteria: specified accuracy reached or max iterations exceeded\n",
        "  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.1)\n",
        "  # flags: how are initial centroids placed\n",
        "  flags = cv2.KMEANS_RANDOM_CENTERS\n",
        "  # train kmeans clustering on the features' descriptors\n",
        "  ### tbh. I have no idea what the third parameter does\n",
        "  ### I copied the code from github, and it had it in there,\n",
        "  ### the documentation of the function only has 5 arguments listed\n",
        "  ### but in the documentation they also use 6 arguments in the examples\n",
        "  print(\"Finding feature clusters\")\n",
        "  compactness, labels, centres = cv2.kmeans(features, k, None, criteria, 10, flags)\n",
        "  print(\"k-means clustering done\")\n",
        "  print(\"Compactness:\", compactness)\n",
        "  print(\"There are\", centres.shape[0], \"\"\"cluster centres, described in\n",
        "  the feature space with\"\"\", centres.shape[1], \"dimensions.\")\n",
        "\n",
        "\n",
        "  ## Examples:\n",
        "  ## When the bag_of_features function is executed on many features, you get an overview on\n",
        "  ## how the clusters are represented.\n",
        "  #print(\"Counts of all clusters on all features:\\n\", bag_of_features(features, centres))\n",
        "  ## With the get_bag_of_feature_assignment you get only the assigned\n",
        "  ## cluster number of a single feature\n",
        "  #print(\"Assigned cluster of feature number 12:\", get_bag_of_feature_assignment(features[12], centres))\n",
        "\n",
        "\n",
        "  # if test data has not been supplied separately from training data\n",
        "  # do a split here and now\n",
        "  if x_test is None or y_test is None:\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "  # Transforming the training data\n",
        "  print(\"Transforming training data\")\n",
        "  # create a list of labels and a list of cluster counts for the training data\n",
        "  x_clusters_train = []\n",
        "  y_sift_train = []\n",
        "  # iterate through all images again\n",
        "  # use enumerate, so that we can get the labels too\n",
        "  for i, img in enumerate(x_train):\n",
        "    # get the SIFT descriptions of the image\n",
        "    # this is a matrix with one row per feature and 128 columns\n",
        "    img_description = CalcFeatures(img)\n",
        "    if img_description is not None:\n",
        "      # assign the image descriptor to clusters, get the counts of those assigmnts\n",
        "      img_clusters = bag_of_features(img_description, centres)\n",
        "      x_clusters_train.append(img_clusters)\n",
        "      # save the label of the current image to the labels list\n",
        "      y_sift_train.append(y_train[i])\n",
        "\n",
        "  # 1 row per image, k columns, each with the count of features of the image,\n",
        "  # that have been assigned to this column's cluster\n",
        "  x_clusters_train = np.vstack(x_clusters_train)\n",
        "\n",
        "\n",
        "\n",
        "  # Transforming the test data\n",
        "  print(\"Transforming test data\")\n",
        "  # create a list of labels and a list of cluster counts for the test data\n",
        "  y_sift_test = []\n",
        "  x_clusters_test = []\n",
        "  # iterate through all images again\n",
        "  # use enumerate, so that we can get the labels too\n",
        "  for i, img in enumerate(x_test):\n",
        "    # get the SIFT descriptions of the image\n",
        "    # this is a matrix with one row per feature and 128 columns\n",
        "    img_description = CalcFeatures(img)\n",
        "    if img_description is not None:\n",
        "      # assign the image descriptor to clusters, get the counts of those assigmnts\n",
        "      img_clusters = bag_of_features(img_description, centres)\n",
        "      x_clusters_test.append(img_clusters)\n",
        "      # save the label of the current image to the labels list\n",
        "      y_sift_test.append(y_test[i])\n",
        "\n",
        "  # 1 row per image, k columns, each with the count of features of the image,\n",
        "  # that have been assigned to this column's cluster\n",
        "  x_clusters_test = np.vstack(x_clusters_test)\n",
        "\n",
        "\n",
        "  time_elapsed = datetime.now() - start_time\n",
        "  print('SIFT-extraction done — time taken (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
        "\n",
        "  # return the transformed data\n",
        "  # if the parameter time_it is true, return the elapsed time as well\n",
        "  if time_it:\n",
        "    return x_clusters_train, y_sift_train, x_clusters_test, y_sift_test, time_elapsed\n",
        "  else:\n",
        "    return x_clusters_train, y_sift_train, x_clusters_test, y_sift_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh3V07Lhlk82"
      },
      "outputs": [],
      "source": [
        "# Simple SVM — training and testing\n",
        "\n",
        "# this function trains on input data and returns certain results\n",
        "# this function uses SVM and returns Balanced Accuracy, weighted F1 and a confusion matrix\n",
        "\n",
        "def SVM_train_test(x_train, y_train, x_test, y_test, print_results=True, time_it=True):\n",
        "  start_time = datetime.now()\n",
        "\n",
        "  # training the classifier\n",
        "  # as they did in the inspiration repo, we will use SVM\n",
        "  model = SVC()\n",
        "  print(\"Training the SVM model\")\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  # make predictions on the training data\n",
        "  print(\"Making predicions on training data\")\n",
        "  preds = model.predict(x_test)\n",
        "  # evaluate the test\n",
        "  acc = balanced_accuracy_score(y_test, preds)\n",
        "  f1 = f1_score(y_test, preds, average=\"weighted\")\n",
        "  conf_mat = confusion_matrix(y_test, preds)\n",
        "\n",
        "  time_elapsed = datetime.now() - start_time\n",
        "  print('SVM training and testing done — time taken (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
        "\n",
        "\n",
        "  if print_results:\n",
        "    print(\"Balanced ccuracy:\", acc)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(conf_mat)\n",
        "\n",
        "  # return the transformed data\n",
        "  # if the parameter time_it is true, return the elapsed time as well\n",
        "  if time_it:\n",
        "    return acc, f1, conf_mat, time_elapsed\n",
        "  else:\n",
        "    return acc, f1, conf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNyva4PSq1ib"
      },
      "source": [
        "### Fashion Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GX6A2W_ejFyi",
        "outputId": "f5347204-9279-47be-afc2-14e487ea801b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIFT feature extraction starting\n",
            "Analysing features of training data\n",
            "241395 features keypoints have been found within the 60000 images.\n",
            "Finding feature clusters\n",
            "k-means clustering done\n",
            "Compactness: 11019539549.760132\n",
            "There are 400 cluster centres, described in\n",
            "  the feature space with 128 dimensions.\n",
            "Transforming training data\n",
            "Transforming test data\n",
            "Training the SVM model\n",
            "Making predicions on training data\n",
            "Balanced ccuracy: 0.6266543916422979\n",
            "F1 Score: 0.6206200579193945\n",
            "[[606  15  62  67  31  39  62   8  47   7]\n",
            " [  3 467  11  91  11  13   8  12  10   2]\n",
            " [ 37  14 497  24 197  27 100   4  49   5]\n",
            " [ 99  60  70 440  69  32  43  37  67   6]\n",
            " [ 18   8 226  38 470  13 129   5  66   0]\n",
            " [  9  12   9  21   2 775  14  91   7  26]\n",
            " [142  18 181  71 158  25 259   7  62   9]\n",
            " [  6  11   6  34   5  46   6 737  18  96]\n",
            " [ 24  21  57  48  41  19  41   9 692   6]\n",
            " [ 14   0   5  18   6  21   2  77  21 824]]\n",
            "SIFT feature extraction starting\n",
            "Analysing features of training data\n",
            "241395 features keypoints have been found within the 60000 images.\n",
            "Finding feature clusters\n",
            "k-means clustering done\n",
            "Compactness: 10837276112.415771\n",
            "There are 600 cluster centres, described in\n",
            "  the feature space with 128 dimensions.\n",
            "Transforming training data\n",
            "Transforming test data\n",
            "Training the SVM model\n",
            "Making predicions on training data\n",
            "Balanced ccuracy: 0.6262652845583003\n",
            "F1 Score: 0.6222066189145283\n",
            "[[623   8  52  71  28  33  62  13  50   4]\n",
            " [  7 444  17 117   9  12   3   4  15   0]\n",
            " [ 29   6 487  36 208  14 113   9  51   1]\n",
            " [ 92  52  75 461  52  31  50  38  66   6]\n",
            " [ 16   5 232  64 444  11 121  16  63   1]\n",
            " [ 15   8   8  29   2 790   5  75   9  25]\n",
            " [137  14 185  61 149  28 259  14  79   6]\n",
            " [  9   8   9  49   3  43   4 735  19  86]\n",
            " [ 23  11  46  53  32  27  42  11 709   4]\n",
            " [ 14   1   6  14   6  13   6  83  23 822]]\n",
            "SIFT feature extraction starting\n",
            "Analysing features of training data\n",
            "241395 features keypoints have been found within the 60000 images.\n",
            "Finding feature clusters\n",
            "k-means clustering done\n",
            "Compactness: 10693564685.044556\n",
            "There are 800 cluster centres, described in\n",
            "  the feature space with 128 dimensions.\n",
            "Transforming training data\n",
            "Transforming test data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-88ffe32d7e99>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# do the SIFT feature extractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mx_sift_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sift_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_sift_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sift_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIFT_feature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# now train and test a model using the SIFT data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-353b0dd76713>\u001b[0m in \u001b[0;36mSIFT_feature_extraction\u001b[0;34m(x_train, y_train, x_test, y_test, k)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg_description\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0;31m# assign the image descriptor to clusters, get the counts of those assigmnts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m       \u001b[0mimg_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mx_clusters_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0;31m# save the label of the current image to the labels list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-17d9bbf92398>\u001b[0m in \u001b[0;36mbag_of_features\u001b[0;34m(features_array, clusters)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdifferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# get the euclidian distances -> sqrt(sum(differences squared))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# get the indices of the sorted distances to get the minimal distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mindex_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# assign data variables\n",
        "x_train = fashion_x_train\n",
        "y_train = fashion_y_train\n",
        "x_test = fashion_x_test\n",
        "y_test = fashion_y_test\n",
        "\n",
        "# do the SIFT feature extractions\n",
        "x_sift_train, y_sift_train, x_sift_test, y_sift_test = SIFT_feature_extraction(x_train, y_train, x_test, y_test, k=400)\n",
        "\n",
        "# now train and test a model using the SIFT data\n",
        "results = SVM_train_test(x_sift_train, y_sift_train, x_sift_test, y_sift_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign data variables\n",
        "x_train = fashion_x_train\n",
        "y_train = fashion_y_train\n",
        "x_test = fashion_x_test\n",
        "y_test = fashion_y_test\n",
        "\n",
        "# extract the features using LBP feature extraction\n",
        "x_lbp_train = lbp_feature_extraction(x_train, time_it=False)\n",
        "x_lbp_test = lbp_feature_extraction(x_test, time_it=False)\n",
        "\n",
        "# now train and test a model using the LBP data\n",
        "results = SVM_train_test(x_lbp_train, y_train, x_lbp_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXtO5JtKDMQM",
        "outputId": "b7ecd674-58d5-45ca-a32d-debd64a1637e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Binary Patterns feature extraction starting\n",
            "LBP-extraction done — time taken (hh:mm:ss.ms) 0:00:34.089099\n",
            "Local Binary Patterns feature extraction starting\n",
            "LBP-extraction done — time taken (hh:mm:ss.ms) 0:00:06.088969\n",
            "Training the SVM model\n",
            "Making predicions on training data\n",
            "Balanced ccuracy: 0.6026\n",
            "F1 Score: 0.5930261753524539\n",
            "[[639   7  71  74  40   5  36   3  78  47]\n",
            " [  1 813  13  40   4  68   1  26  30   4]\n",
            " [ 17   1 629   7 204   1  74   0  56  11]\n",
            " [ 36  42  31 548  58  60  20 117  48  40]\n",
            " [ 14   1 233  70 517   5 100   3  54   3]\n",
            " [  4 120   4  13   4 665   0 130  19  41]\n",
            " [138   8 332  42 185   4 173   2 100  16]\n",
            " [  0  39   0  54   0  47   0 787   3  70]\n",
            " [ 63  35 137  51  62  26  46  19 487  74]\n",
            " [ 46   2   5  57   4   8   2  83  25 768]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwN7Cx5Bq-Gk"
      },
      "source": [
        "### Faces in the wild"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "29JanFh9q9kc",
        "outputId": "24674076-d16d-4a50-e1a4-efe2b1960843"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'faces_x_bw' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-606642ec0f71>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# assign data variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaces_x_bw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaces_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'faces_x_bw' is not defined"
          ]
        }
      ],
      "source": [
        "# assign data variables\n",
        "x_train = faces_x_bw\n",
        "y_train = faces_y\n",
        "\n",
        "\n",
        "# do the SIFT feature extractions\n",
        "x_sift_train, y_sift_train, x_sift_test, y_sift_test = SIFT_feature_extraction(x_train, y_train, k=400)\n",
        "\n",
        "# now train and test a model using the SIFT data\n",
        "results = SVM_train_test(x_sift_train, y_sift_train, x_sift_test, y_sift_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign data variables\n",
        "x_train = faces_x_bw\n",
        "y_train = faces_y\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train)\n",
        "\n",
        "# extract the features using LBP feature extraction\n",
        "x_lbp_train = lbp_feature_extraction(x_train, time_it=False)\n",
        "x_lbp_test = lbp_feature_extraction(x_test, time_it=False)\n",
        "\n",
        "# now train and test a model using the LBP data\n",
        "results = SVM_train_test(x_lbp_train, y_train, x_lbp_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyYuh-H4NP-z",
        "outputId": "69522d19-4670-4e55-e169-85b55a088ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Binary Patterns feature extraction starting\n",
            "LBP-extraction done — time taken (hh:mm:ss.ms) 0:03:52.409498\n",
            "Local Binary Patterns feature extraction starting\n",
            "LBP-extraction done — time taken (hh:mm:ss.ms) 0:01:15.242728\n",
            "Training the SVM model\n",
            "Making predicions on training data\n",
            "SVM training and testing done — time taken (hh:mm:ss.ms) 0:02:11.680008\n",
            "Balanced ccuracy: 0.0009775171065493646\n",
            "F1 Score: 0.00627768261133438\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "888Je7mYN8x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet\n",
        "https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide"
      ],
      "metadata": {
        "id": "8cMNby-wOB5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lenet(input_shape, num_classes, optimizer, loss, metrics):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the feature maps\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(120, activation='relu'))\n",
        "    model.add(Dense(84, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TbJbTjBDQh1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet V2\n",
        "https://www.tensorflow.org/tutorials/images/transfer_learning \\\n",
        "expects pixel values in [-1, 1]"
      ],
      "metadata": {
        "id": "LIq8MzKkUp95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_nHe2VwtUuh"
      },
      "outputs": [],
      "source": [
        "def build_mobilenetv2(input_shape, num_classes, optimizer, loss, metrics):\n",
        "    # Load the pre-trained MobileNetV2 model\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "    # Create a new model on top of the base model\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing for CNN\n",
        "prefetching?, normalization, data augmentation"
      ],
      "metadata": {
        "id": "VRcVVSb6YkuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Normalisation\n",
        "MobileNet V2 expects"
      ],
      "metadata": {
        "id": "QojOmuWqbi5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_mobilenet(images):\n",
        "    # Scale pixel values to the range [-1, 1]\n",
        "    normalized_images = (images / 127.5) - 1.0\n",
        "    return normalized_images\n",
        "def normalize_lenet(images):\n",
        "    # Scale pixel values to the range [0, 1]\n",
        "    normalized_images = (images / 255)\n",
        "    return normalized_images\n"
      ],
      "metadata": {
        "id": "KkYQXdg5pBgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation"
      ],
      "metadata": {
        "id": "YHdLeobSboAi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}